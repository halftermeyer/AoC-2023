{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 12\n",
    "\n",
    "- part 1 with Neo4j and QPPs\n",
    "- part 1 with a bad memoized recursive approach - part 2 does not scale\n",
    "- part 1 and 2 with a neater fully recursive approach\n",
    "\n",
    "## Neo4j and QPPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "gds = GraphDataScience(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "# Check the installed GDS version on the server\n",
    "print(gds.version())\n",
    "assert gds.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"input.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(input_file, 'r')\n",
    "for ix, line in enumerate(file):\n",
    "    row, groups =line.split(' ')\n",
    "    groups = [int(g) for g in groups.split(',')]\n",
    "    qpp = '\\n'.join([f\"MATCH (head:Head WHERE head.row_id={ix})\",\n",
    "    \"(()-->(:OK|UNKNOWN))*\",\n",
    "    \"(()-->(:OK|UNKNOWN))+\\n\".join([f\"(()-->(g_{jx}:NOK|UNKNOWN)){{{g}}}\" for jx, g in enumerate(groups)]),\n",
    "    \"(()-->(:OK|UNKNOWN|Tail))*(:Tail)\",\n",
    "    \"WITH DISTINCT head, \"+', '.join([f\"g_{jx}\" for jx, _ in enumerate(groups)]),\n",
    "    \"WITH head, count(*) as matches\",\n",
    "    \"SET head.matches = matches\"])\n",
    "    gds.run_cypher(\"\"\"\n",
    "    CREATE (:Line {id:$ix, row:$row, groups:$groups, qpp:$qpp})\n",
    "    \"\"\", {'ix':ix, 'row':row, 'groups':groups, 'qpp':qpp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, row in gds.run_cypher(\"\"\"MATCH (line:Line) RETURN line.id AS id, line.row AS row\"\"\").itertuples(index=False):\n",
    "    gds.run_cypher(\"\"\"\n",
    "    WITH $row AS row\n",
    "    UNWIND split(row, '') AS c\n",
    "    CREATE (s:Spring {row_id:$id, state:c})\n",
    "    WITH collect(s) AS springs\n",
    "    CALL apoc.nodes.link(springs, 'NEXT')\n",
    "    \"\"\", {'row':row, 'id':id})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = \"\"\"MATCH (s:Spring)\n",
    "WHERE NOT EXISTS {()-[:NEXT]->(s)}\n",
    "SET s:First;\n",
    "MATCH (s:Spring)\n",
    "WHERE NOT EXISTS {(s)-[:NEXT]->()}\n",
    "SET s:Last;\n",
    "MATCH (s:Spring {state:'.'})\n",
    "SET s:OK;\n",
    "MATCH (s:Spring {state:'#'})\n",
    "SET s:NOK;\n",
    "MATCH (s:Spring {state:'?'})\n",
    "SET s:UNKNOWN;\n",
    "MATCH (f:First)\n",
    "MERGE (h:Head {row_id:f.row_id})\n",
    "MERGE (h)-[:NEXT]->(f);\n",
    "MATCH (l:Last)\n",
    "MERGE (t:Tail {row_id:l.row_id})\n",
    "MERGE (l)-[:NEXT]->(t)\n",
    "\"\"\".split(';')\n",
    "for query in queries:\n",
    "    gds.run_cypher(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.run_cypher(\"\"\"CALL apoc.periodic.iterate('MATCH (l:Line) RETURN l.qpp AS qpp',\n",
    "'CALL apoc.cypher.runWrite(qpp,{}) YIELD value RETURN value.count AS count',\n",
    "{parallel:TRUE})\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.run_cypher(\"\"\"MATCH (h:Head)\n",
    "RETURN sum(h.matches) AS part1\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Ugly dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache\n",
    "def arrangements(chunk, groups):\n",
    "    #print(prefix,'call ', chunk, groups) \n",
    "    res = set()\n",
    "\n",
    "    if len(chunk) == 0 and len(groups) > 0:\n",
    "        return set()\n",
    "\n",
    "    if len(groups) == 0:\n",
    "        if '#' in chunk:\n",
    "            return set()\n",
    "        res_nosearch = set(['.'*len(chunk)])\n",
    "        #print(prefix,'no search', res_nosearch)\n",
    "        return res_nosearch\n",
    "\n",
    "    size, *others = groups\n",
    "    others = tuple(others)\n",
    "    fifo = chunk\n",
    "\n",
    "    skipped = 0\n",
    "    while len(fifo) > 0:\n",
    "        candidate = fifo[0:size]\n",
    "        if len(candidate) < size:\n",
    "            break\n",
    "        if len(fifo) > size:\n",
    "            next_char = fifo[size]\n",
    "        else:\n",
    "            next_char = ''\n",
    "        if not '.' in candidate and next_char != '#':\n",
    "            #print(prefix,'skipped',skipped,'size',size,'next c',next_char)\n",
    "            start = \".\"*skipped + \"#\"*size + ('.' if next_char != '' else '')\n",
    "            #print(prefix,\"start\",start) \n",
    "            for end in arrangements(fifo[size+1:], others):\n",
    "                #print(prefix,\"add\",start + end) \n",
    "                res.add(start + end)\n",
    "        skip, fifo = fifo[0], fifo[1:]\n",
    "        skipped += 1\n",
    "        if len(fifo) < size:\n",
    "            break\n",
    "        if skip == \"#\":\n",
    "            break\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6935"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = \"input.txt\"\n",
    "file = open(input_file, 'r')\n",
    "part1 = 0\n",
    "for ix, line in enumerate(file):\n",
    "    row, groups =line.split(' ')\n",
    "    groups = tuple([int(g) for g in groups.split(',')])\n",
    "    arr = len(arrangements(row,groups))\n",
    "    part1 += arr\n",
    "part1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That won't scale - DON'T RUN THIS\n",
    "#input_file = \"input.txt\"\n",
    "#file = open(input_file, 'r')\n",
    "#part2 = 0\n",
    "#for ix, line in enumerate(file):\n",
    "#    row, groups =line.split(' ')\n",
    "#    row = '?'.join([row]*5)\n",
    "#    groups = groups.strip()\n",
    "#    groups = ','.join([groups]*5)\n",
    "#    groups = tuple([int(g) for g in groups.split(',')])\n",
    "#    arr = len(arrangements(row,groups))\n",
    "#    part2 += arr\n",
    "#part2\n",
    "# To slow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neat dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@cache\n",
    "def nb_arrangements( chunk, groups ):\n",
    "\n",
    "    if len(groups) == 0: # work is done unless unmatched '#' left\n",
    "        return 0 if '#' in chunk else 1\n",
    "    if sum(groups) + len(groups) - 1 > len(chunk): # no room to end work\n",
    "        return 0\n",
    "\n",
    "    # FULLY RECURSIVE APPROACH\n",
    "    if chunk[0] == '.':\n",
    "        return nb_arrangements( chunk[1:], groups)\n",
    "\n",
    "    cpt = 0\n",
    "    if chunk[0] == '?': # considering the case it's a dammaged spring (like '.')\n",
    "        cpt += nb_arrangements( chunk[1:], groups)\n",
    "\n",
    "    group = groups[0]\n",
    "    if '.' not in chunk[:group] and (len(chunk) == group or (len(chunk) > group and chunk[group] != '#')):\n",
    "            # placing the first group of # is possible.\n",
    "            # Let's consider it's done and count following arrangements\n",
    "            cpt += nb_arrangements( chunk[group+1:], groups[1:])\n",
    "\n",
    "    return cpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6935"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part1 = 0\n",
    "with open('input.txt', 'r') as f:\n",
    "    for line in f.read().splitlines():\n",
    "        row, groups = line.split(' ')\n",
    "        groups = [int(x) for x in groups.split(',')]\n",
    "        part1 += nb_arrangements( row , tuple(groups) ) # tuples pour pouvoir hasher...\n",
    "\n",
    "part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3920437278260"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part2 = 0\n",
    "with open('input.txt', 'r') as f:\n",
    "    for line in f.read().splitlines():\n",
    "        chunk, groups = line.split(' ')\n",
    "        chunk = '?'.join([chunk]*5)\n",
    "        groups = ','.join([groups]*5)\n",
    "        groups = [int(x) for x in groups.split(',')]\n",
    "        part2 += nb_arrangements( chunk , tuple(groups) )\n",
    "\n",
    "part2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
